{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mx.mod.Module.load('/home/wuyuxiang/quan/insightface/model_params/theBest', 0, context=mx.gpu(0), label_names=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.bind(data_shapes=[('data', (batch_size, 3, 112, 112))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tifs = glob.glob('/home/wuyuxiang/cvpr/caspeal/*.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = glob.glob('/home/wuyuxiang/cvpr/caspeal/*.pdf')\n",
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.prod(cv2.imread(tifs[2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(tifs[100])[120:]\n",
    "img = cv2.resize(img, (112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(feature1 * feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = (img-127.5)*0.0078125\n",
    "img = np.transpose(img, (2,0,1))[np.newaxis, :, :, :]\n",
    "img = mx.nd.array(img) \n",
    "db = mx.io.DataBatch(data=(img,), provide_data=[('data', (1, 3, 112, 112))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(db, is_train=False)\n",
    "feature = model.get_outputs()[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = np.zeros((batch_size, 512))\n",
    "data = np.zeros((batch_size, 3, 112, 112))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    img = cv2.imread(tifs[i])[120:]\n",
    "    img = cv2.resize(img, (112, 112))\n",
    "    img = (img-127.5)*0.0078125\n",
    "    img = np.transpose(img, (2,0,1))[np.newaxis, :, :, :]\n",
    "    data[i] = img\n",
    "data = mx.nd.array(data)\n",
    "db = mx.io.DataBatch(data=(data,), provide_data=[('data', (batch_size, 3, 112, 112))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(db, is_train=False)\n",
    "feature = model.get_outputs()[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_fea = feature / np.linalg.norm(feature, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(feature, feature.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_list = []\n",
    "for flip in [0, 1]:\n",
    "    data = mx.nd.empty((len(issame_list) * 2, 3, 112, 112))\n",
    "    data_list.append(data)\n",
    "for i in range(len(issame_list) * 2):\n",
    "    _bin = bins[i]\n",
    "    img = mx.image.imdecode(_bin)\n",
    "    img = mx.nd.transpose(img, axes=(2, 0, 1))\n",
    "    for flip in [0, 1]:\n",
    "        if flip == 1:\n",
    "            img = mx.nd.flip(data=img, axis=2)\n",
    "        data_list[flip][i] = img\n",
    "    if i % 1000 == 0:\n",
    "        print('loading bin', i)\n",
    "print(data_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 : 6000\n"
     ]
    }
   ],
   "source": [
    "path = '/home/wuyuxiang/faces_ms1m_112x112/' + 'agedb_30.bin'\n",
    "bins, issame_list = pickle.load(open(path, 'rb'))\n",
    "print('{} : {}'.format(len(bins), len(issame_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000 : 6000\n"
     ]
    }
   ],
   "source": [
    "path = '/home/wuyuxiang/faces_ms1m_112x112/' + 'lfw.bin'\n",
    "bins, issame_list = pickle.load(open(path, 'rb'))\n",
    "print('{} : {}'.format(len(bins), len(issame_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000 : 7000\n"
     ]
    }
   ],
   "source": [
    "path = '/home/wuyuxiang/faces_ms1m_112x112/' + 'cfp_fp.bin'\n",
    "bins, issame_list = pickle.load(open(path, 'rb'))\n",
    "print('{} : {}'.format(len(bins), len(issame_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(data[-1], bins[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_list[0][:batch_size]\n",
    "db = mx.io.DataBatch(data=(data,), provide_data=[('data', (batch_size, 3, 112, 112))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(db, is_train=False)\n",
    "feature = model.get_outputs()[0].asnumpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = issame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = [True] * (len(a) * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[1::2] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0::2] = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding1 = embedding[0::2]\n",
    "embedding2 = embedding[1::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.sum(np.square(embedding1 - embedding2), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.less(result, 1.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issame_list[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import interpolate\n",
    "\n",
    "x = np.arange(0,10)\n",
    "y = np.exp(-x/3.0)\n",
    "f = interpolate.interp1d(x, y, kind='cubic')\n",
    "\n",
    "xnew = np.arange(0,9,0.1)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y,'o',xnew,f(xnew),'-')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = (issame_list[batch_size*17:batch_size*18-50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real[2]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict  = issame_list[batch_size*17:batch_size*18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict[0] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_val_far(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    #print(true_accept, false_accept)\n",
    "    #print(n_same, n_diff)\n",
    "    val = float(true_accept) / float(n_same + 1e-10)\n",
    "    far = float(false_accept) / float(n_diff + 1e-10)\n",
    "    return val, far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_val_far(0.0, result, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_val(thresholds, embeddings1, embeddings2, actual_issame, far_target, nrof_folds=2):\n",
    "    assert(embeddings1.shape[0] == embeddings2.shape[0])\n",
    "    assert(embeddings1.shape[1] == embeddings2.shape[1])\n",
    "    nrof_pairs = min(len(actual_issame), embeddings1.shape[0])\n",
    "    nrof_thresholds = len(thresholds)\n",
    "    k_fold = LFold(n_splits=nrof_folds, shuffle=False)\n",
    "    \n",
    "    val = np.zeros(nrof_folds)\n",
    "    far = np.zeros(nrof_folds)\n",
    "    \n",
    "    diff = np.subtract(embeddings1, embeddings2)\n",
    "    dist = np.sum(np.square(diff),1)\n",
    "    indices = np.arange(nrof_pairs)\n",
    "    \n",
    "    for fold_idx, (train_set, test_set) in enumerate(k_fold.split(indices)):\n",
    "        # Find the threshold that gives FAR = far_target\n",
    "        far_train = np.zeros(nrof_thresholds)\n",
    "        for threshold_idx, threshold in enumerate(thresholds):\n",
    "            \n",
    "            _, far_train[threshold_idx] = calculate_val_far(threshold, dist[train_set], actual_issame[train_set[0]:train_set[0]+len(train_set)])\n",
    "\n",
    "        if np.max(far_train)>=far_target:\n",
    "            f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "            threshold = f(far_target)\n",
    "        else:\n",
    "            threshold = 0.0\n",
    "    \n",
    "        val[fold_idx], far[fold_idx] = calculate_val_far(threshold, dist[test_set], actual_issame[test_set[0]:test_set[0]+len(test_set)])\n",
    "  \n",
    "    val_mean = np.mean(val)\n",
    "    far_mean = np.mean(far)\n",
    "    val_std = np.std(val)\n",
    "    return val_mean, val_std, far_mean\n",
    "calculate_val(thresholds, embedding1, embedding2, issame_list[:batch_size/2], 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 4, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from numpy import transpose\n",
    "import mxnet as mx\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class VerificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "       LFW: 12K\n",
    "       AgeDB-30: 12K\n",
    "       CFP-FP: 14K\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = [path + 'lfw.bin', path + 'agedb_30.bin', path + 'cfp_fp.bin']\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for cur in self.path:\n",
    "            with open(cur, 'rb') as f:\n",
    "                data, label = pickle.load(f)\n",
    "                self.data.extend(data)\n",
    "                self.label.extend(label)\n",
    "        self.imgs = len(self.data)\n",
    "        self.people = len(self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ = mx.image.imdecode(self.data[idx]).asnumpy()\n",
    "        img_ = np.transpose(img_, axes=(2, 0, 1))\n",
    "        return torch.tensor(img_, dtype=torch.float32), torch.tensor([self.label[idx // 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = VerificationDataset('/home/wuyuxiang/faces_ms1m_112x112/')\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data : 20000\n",
      "loading data : 0\n",
      "loading data : 15000\n",
      "loading data : 30000\n",
      "loading data : 35000\n",
      "loading data : 5000\n",
      "loading data : 25000\n",
      "loading data : 10000\n"
     ]
    }
   ],
   "source": [
    "for i, (data, target) in enumerate((train_loader)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3800"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# -*- encoding:utf-8 -*- \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import mxnet as mx\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class VerificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "       LFW: 12K\n",
    "       AgeDB-30: 12K\n",
    "       CFP-FP: 14K\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = [path + 'lfw.bin', path + 'agedb_30.bin', path + 'cfp_fp.bin']\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        for cur in self.path:\n",
    "            with open(cur, 'rb') as f:\n",
    "                data, label = pickle.load(f, encoding='bytes')\n",
    "                self.data.extend(data)\n",
    "                self.label.extend(label)\n",
    "        self.imgs = len(self.data)\n",
    "        self.people = len(self.label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.imgs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_ = mx.image.imdecode(self.data[idx]).asnumpy()\n",
    "        img_ = np.transpose(img_, axes=(2, 0, 1))\n",
    "        return torch.tensor(img_, dtype=torch.float32), torch.tensor([self.label[idx // 2]])\n",
    "\n",
    "\n",
    "dataset = VerificationDataset('/home/wuyuxiang/faces_ms1m_112x112/')\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=10, shuffle=True)\n",
    "print(True)\n",
    "for i, (data, target) in enumerate(train_loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 0, 0, 0, 0, 1, 1, 1], dtype=torch.uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-21-22e5ed7ba28f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-21-22e5ed7ba28f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    True == 2\\1\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "True == 2\\1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1111,  111,    1,    1,    1,    1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(torch.tensor([1111,111,1,1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = '/home/cortex/datashare/ijb/IJB/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ijb_a = glob.glob(path+'IJB-A/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ijb_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-38d4b0363d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load('../../torch_face_params/resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'odict_keys' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f9c9aaeccc31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'odict_keys' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../home/wuyuxiang/quan/FaceRecognition/networks/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"res0.weight\", \"res1.0.fx.0.weight\", \"res1.0.fx.0.bias\", \"res1.0.fx.0.running_mean\", \"res1.0.fx.0.running_var\", \"res1.0.fx.1.weight\", \"res1.0.fx.2.weight\", \"res1.0.fx.3.weight\", \"res1.0.gx.0.weight\", \"res1.1.fx.0.weight\", \"res1.1.fx.0.bias\", \"res1.1.fx.0.running_mean\", \"res1.1.fx.0.running_var\", \"res1.1.fx.1.weight\", \"res1.1.fx.2.weight\", \"res1.1.fx.3.weight\", \"res1.2.fx.0.weight\", \"res1.2.fx.0.bias\", \"res1.2.fx.0.running_mean\", \"res1.2.fx.0.running_var\", \"res1.2.fx.1.weight\", \"res1.2.fx.2.weight\", \"res1.2.fx.3.weight\", \"res2.0.fx.0.weight\", \"res2.0.fx.0.bias\", \"res2.0.fx.0.running_mean\", \"res2.0.fx.0.running_var\", \"res2.0.fx.1.weight\", \"res2.0.fx.2.weight\", \"res2.0.fx.3.weight\", \"res2.0.gx.0.weight\", \"res2.1.fx.0.weight\", \"res2.1.fx.0.bias\", \"res2.1.fx.0.running_mean\", \"res2.1.fx.0.running_var\", \"res2.1.fx.1.weight\", \"res2.1.fx.2.weight\", \"res2.1.fx.3.weight\", \"res2.2.fx.0.weight\", \"res2.2.fx.0.bias\", \"res2.2.fx.0.running_mean\", \"res2.2.fx.0.running_var\", \"res2.2.fx.1.weight\", \"res2.2.fx.2.weight\", \"res2.2.fx.3.weight\", \"res2.3.fx.0.weight\", \"res2.3.fx.0.bias\", \"res2.3.fx.0.running_mean\", \"res2.3.fx.0.running_var\", \"res2.3.fx.1.weight\", \"res2.3.fx.2.weight\", \"res2.3.fx.3.weight\", \"res3.0.fx.0.weight\", \"res3.0.fx.0.bias\", \"res3.0.fx.0.running_mean\", \"res3.0.fx.0.running_var\", \"res3.0.fx.1.weight\", \"res3.0.fx.2.weight\", \"res3.0.fx.3.weight\", \"res3.0.gx.0.weight\", \"res3.1.fx.0.weight\", \"res3.1.fx.0.bias\", \"res3.1.fx.0.running_mean\", \"res3.1.fx.0.running_var\", \"res3.1.fx.1.weight\", \"res3.1.fx.2.weight\", \"res3.1.fx.3.weight\", \"res3.2.fx.0.weight\", \"res3.2.fx.0.bias\", \"res3.2.fx.0.running_mean\", \"res3.2.fx.0.running_var\", \"res3.2.fx.1.weight\", \"res3.2.fx.2.weight\", \"res3.2.fx.3.weight\", \"res3.3.fx.0.weight\", \"res3.3.fx.0.bias\", \"res3.3.fx.0.running_mean\", \"res3.3.fx.0.running_var\", \"res3.3.fx.1.weight\", \"res3.3.fx.2.weight\", \"res3.3.fx.3.weight\", \"res3.4.fx.0.weight\", \"res3.4.fx.0.bias\", \"res3.4.fx.0.running_mean\", \"res3.4.fx.0.running_var\", \"res3.4.fx.1.weight\", \"res3.4.fx.2.weight\", \"res3.4.fx.3.weight\", \"res3.5.fx.0.weight\", \"res3.5.fx.0.bias\", \"res3.5.fx.0.running_mean\", \"res3.5.fx.0.running_var\", \"res3.5.fx.1.weight\", \"res3.5.fx.2.weight\", \"res3.5.fx.3.weight\", \"res3.6.fx.0.weight\", \"res3.6.fx.0.bias\", \"res3.6.fx.0.running_mean\", \"res3.6.fx.0.running_var\", \"res3.6.fx.1.weight\", \"res3.6.fx.2.weight\", \"res3.6.fx.3.weight\", \"res3.7.fx.0.weight\", \"res3.7.fx.0.bias\", \"res3.7.fx.0.running_mean\", \"res3.7.fx.0.running_var\", \"res3.7.fx.1.weight\", \"res3.7.fx.2.weight\", \"res3.7.fx.3.weight\", \"res3.8.fx.0.weight\", \"res3.8.fx.0.bias\", \"res3.8.fx.0.running_mean\", \"res3.8.fx.0.running_var\", \"res3.8.fx.1.weight\", \"res3.8.fx.2.weight\", \"res3.8.fx.3.weight\", \"res3.9.fx.0.weight\", \"res3.9.fx.0.bias\", \"res3.9.fx.0.running_mean\", \"res3.9.fx.0.running_var\", \"res3.9.fx.1.weight\", \"res3.9.fx.2.weight\", \"res3.9.fx.3.weight\", \"res3.10.fx.0.weight\", \"res3.10.fx.0.bias\", \"res3.10.fx.0.running_mean\", \"res3.10.fx.0.running_var\", \"res3.10.fx.1.weight\", \"res3.10.fx.2.weight\", \"res3.10.fx.3.weight\", \"res3.11.fx.0.weight\", \"res3.11.fx.0.bias\", \"res3.11.fx.0.running_mean\", \"res3.11.fx.0.running_var\", \"res3.11.fx.1.weight\", \"res3.11.fx.2.weight\", \"res3.11.fx.3.weight\", \"res3.12.fx.0.weight\", \"res3.12.fx.0.bias\", \"res3.12.fx.0.running_mean\", \"res3.12.fx.0.running_var\", \"res3.12.fx.1.weight\", \"res3.12.fx.2.weight\", \"res3.12.fx.3.weight\", \"res3.13.fx.0.weight\", \"res3.13.fx.0.bias\", \"res3.13.fx.0.running_mean\", \"res3.13.fx.0.running_var\", \"res3.13.fx.1.weight\", \"res3.13.fx.2.weight\", \"res3.13.fx.3.weight\", \"res4.0.fx.0.weight\", \"res4.0.fx.0.bias\", \"res4.0.fx.0.running_mean\", \"res4.0.fx.0.running_var\", \"res4.0.fx.1.weight\", \"res4.0.fx.2.weight\", \"res4.0.fx.3.weight\", \"res4.0.gx.0.weight\", \"res4.1.fx.0.weight\", \"res4.1.fx.0.bias\", \"res4.1.fx.0.running_mean\", \"res4.1.fx.0.running_var\", \"res4.1.fx.1.weight\", \"res4.1.fx.2.weight\", \"res4.1.fx.3.weight\", \"res4.2.fx.0.weight\", \"res4.2.fx.0.bias\", \"res4.2.fx.0.running_mean\", \"res4.2.fx.0.running_var\", \"res4.2.fx.1.weight\", \"res4.2.fx.2.weight\", \"res4.2.fx.3.weight\", \"end_block1.0.weight\", \"end_block1.0.bias\", \"end_block1.0.running_mean\", \"end_block1.0.running_var\", \"end_block2.0.weight\", \"end_block2.0.bias\", \"end_block2.1.weight\", \"end_block2.1.bias\", \"end_block2.1.running_mean\", \"end_block2.1.running_var\". \n\tUnexpected key(s) in state_dict: \"embedding.res0.weight\", \"embedding.res1.0.fx.0.weight\", \"embedding.res1.0.fx.0.bias\", \"embedding.res1.0.fx.0.running_mean\", \"embedding.res1.0.fx.0.running_var\", \"embedding.res1.0.fx.0.num_batches_tracked\", \"embedding.res1.0.fx.1.weight\", \"embedding.res1.0.fx.3.weight\", \"embedding.res1.0.gx.0.weight\", \"embedding.res1.1.fx.0.weight\", \"embedding.res1.1.fx.0.bias\", \"embedding.res1.1.fx.0.running_mean\", \"embedding.res1.1.fx.0.running_var\", \"embedding.res1.1.fx.0.num_batches_tracked\", \"embedding.res1.1.fx.1.weight\", \"embedding.res1.1.fx.3.weight\", \"embedding.res1.2.fx.0.weight\", \"embedding.res1.2.fx.0.bias\", \"embedding.res1.2.fx.0.running_mean\", \"embedding.res1.2.fx.0.running_var\", \"embedding.res1.2.fx.0.num_batches_tracked\", \"embedding.res1.2.fx.1.weight\", \"embedding.res1.2.fx.3.weight\", \"embedding.res2.0.fx.0.weight\", \"embedding.res2.0.fx.0.bias\", \"embedding.res2.0.fx.0.running_mean\", \"embedding.res2.0.fx.0.running_var\", \"embedding.res2.0.fx.0.num_batches_tracked\", \"embedding.res2.0.fx.1.weight\", \"embedding.res2.0.fx.3.weight\", \"embedding.res2.0.gx.0.weight\", \"embedding.res2.1.fx.0.weight\", \"embedding.res2.1.fx.0.bias\", \"embedding.res2.1.fx.0.running_mean\", \"embedding.res2.1.fx.0.running_var\", \"embedding.res2.1.fx.0.num_batches_tracked\", \"embedding.res2.1.fx.1.weight\", \"embedding.res2.1.fx.3.weight\", \"embedding.res2.2.fx.0.weight\", \"embedding.res2.2.fx.0.bias\", \"embedding.res2.2.fx.0.running_mean\", \"embedding.res2.2.fx.0.running_var\", \"embedding.res2.2.fx.0.num_batches_tracked\", \"embedding.res2.2.fx.1.weight\", \"embedding.res2.2.fx.3.weight\", \"embedding.res2.3.fx.0.weight\", \"embedding.res2.3.fx.0.bias\", \"embedding.res2.3.fx.0.running_mean\", \"embedding.res2.3.fx.0.running_var\", \"embedding.res2.3.fx.0.num_batches_tracked\", \"embedding.res2.3.fx.1.weight\", \"embedding.res2.3.fx.3.weight\", \"embedding.res3.0.fx.0.weight\", \"embedding.res3.0.fx.0.bias\", \"embedding.res3.0.fx.0.running_mean\", \"embedding.res3.0.fx.0.running_var\", \"embedding.res3.0.fx.0.num_batches_tracked\", \"embedding.res3.0.fx.1.weight\", \"embedding.res3.0.fx.3.weight\", \"embedding.res3.0.gx.0.weight\", \"embedding.res3.1.fx.0.weight\", \"embedding.res3.1.fx.0.bias\", \"embedding.res3.1.fx.0.running_mean\", \"embedding.res3.1.fx.0.running_var\", \"embedding.res3.1.fx.0.num_batches_tracked\", \"embedding.res3.1.fx.1.weight\", \"embedding.res3.1.fx.3.weight\", \"embedding.res3.2.fx.0.weight\", \"embedding.res3.2.fx.0.bias\", \"embedding.res3.2.fx.0.running_mean\", \"embedding.res3.2.fx.0.running_var\", \"embedding.res3.2.fx.0.num_batches_tracked\", \"embedding.res3.2.fx.1.weight\", \"embedding.res3.2.fx.3.weight\", \"embedding.res3.3.fx.0.weight\", \"embedding.res3.3.fx.0.bias\", \"embedding.res3.3.fx.0.running_mean\", \"embedding.res3.3.fx.0.running_var\", \"embedding.res3.3.fx.0.num_batches_tracked\", \"embedding.res3.3.fx.1.weight\", \"embedding.res3.3.fx.3.weight\", \"embedding.res3.4.fx.0.weight\", \"embedding.res3.4.fx.0.bias\", \"embedding.res3.4.fx.0.running_mean\", \"embedding.res3.4.fx.0.running_var\", \"embedding.res3.4.fx.0.num_batches_tracked\", \"embedding.res3.4.fx.1.weight\", \"embedding.res3.4.fx.3.weight\", \"embedding.res3.5.fx.0.weight\", \"embedding.res3.5.fx.0.bias\", \"embedding.res3.5.fx.0.running_mean\", \"embedding.res3.5.fx.0.running_var\", \"embedding.res3.5.fx.0.num_batches_tracked\", \"embedding.res3.5.fx.1.weight\", \"embedding.res3.5.fx.3.weight\", \"embedding.res3.6.fx.0.weight\", \"embedding.res3.6.fx.0.bias\", \"embedding.res3.6.fx.0.running_mean\", \"embedding.res3.6.fx.0.running_var\", \"embedding.res3.6.fx.0.num_batches_tracked\", \"embedding.res3.6.fx.1.weight\", \"embedding.res3.6.fx.3.weight\", \"embedding.res3.7.fx.0.weight\", \"embedding.res3.7.fx.0.bias\", \"embedding.res3.7.fx.0.running_mean\", \"embedding.res3.7.fx.0.running_var\", \"embedding.res3.7.fx.0.num_batches_tracked\", \"embedding.res3.7.fx.1.weight\", \"embedding.res3.7.fx.3.weight\", \"embedding.res3.8.fx.0.weight\", \"embedding.res3.8.fx.0.bias\", \"embedding.res3.8.fx.0.running_mean\", \"embedding.res3.8.fx.0.running_var\", \"embedding.res3.8.fx.0.num_batches_tracked\", \"embedding.res3.8.fx.1.weight\", \"embedding.res3.8.fx.3.weight\", \"embedding.res3.9.fx.0.weight\", \"embedding.res3.9.fx.0.bias\", \"embedding.res3.9.fx.0.running_mean\", \"embedding.res3.9.fx.0.running_var\", \"embedding.res3.9.fx.0.num_batches_tracked\", \"embedding.res3.9.fx.1.weight\", \"embedding.res3.9.fx.3.weight\", \"embedding.res3.10.fx.0.weight\", \"embedding.res3.10.fx.0.bias\", \"embedding.res3.10.fx.0.running_mean\", \"embedding.res3.10.fx.0.running_var\", \"embedding.res3.10.fx.0.num_batches_tracked\", \"embedding.res3.10.fx.1.weight\", \"embedding.res3.10.fx.3.weight\", \"embedding.res3.11.fx.0.weight\", \"embedding.res3.11.fx.0.bias\", \"embedding.res3.11.fx.0.running_mean\", \"embedding.res3.11.fx.0.running_var\", \"embedding.res3.11.fx.0.num_batches_tracked\", \"embedding.res3.11.fx.1.weight\", \"embedding.res3.11.fx.3.weight\", \"embedding.res3.12.fx.0.weight\", \"embedding.res3.12.fx.0.bias\", \"embedding.res3.12.fx.0.running_mean\", \"embedding.res3.12.fx.0.running_var\", \"embedding.res3.12.fx.0.num_batches_tracked\", \"embedding.res3.12.fx.1.weight\", \"embedding.res3.12.fx.3.weight\", \"embedding.res3.13.fx.0.weight\", \"embedding.res3.13.fx.0.bias\", \"embedding.res3.13.fx.0.running_mean\", \"embedding.res3.13.fx.0.running_var\", \"embedding.res3.13.fx.0.num_batches_tracked\", \"embedding.res3.13.fx.1.weight\", \"embedding.res3.13.fx.3.weight\", \"embedding.res4.0.fx.0.weight\", \"embedding.res4.0.fx.0.bias\", \"embedding.res4.0.fx.0.running_mean\", \"embedding.res4.0.fx.0.running_var\", \"embedding.res4.0.fx.0.num_batches_tracked\", \"embedding.res4.0.fx.1.weight\", \"embedding.res4.0.fx.3.weight\", \"embedding.res4.0.gx.0.weight\", \"embedding.res4.1.fx.0.weight\", \"embedding.res4.1.fx.0.bias\", \"embedding.res4.1.fx.0.running_mean\", \"embedding.res4.1.fx.0.running_var\", \"embedding.res4.1.fx.0.num_batches_tracked\", \"embedding.res4.1.fx.1.weight\", \"embedding.res4.1.fx.3.weight\", \"embedding.res4.2.fx.0.weight\", \"embedding.res4.2.fx.0.bias\", \"embedding.res4.2.fx.0.running_mean\", \"embedding.res4.2.fx.0.running_var\", \"embedding.res4.2.fx.0.num_batches_tracked\", \"embedding.res4.2.fx.1.weight\", \"embedding.res4.2.fx.3.weight\", \"embedding.end_block1.0.weight\", \"embedding.end_block1.0.bias\", \"embedding.end_block1.0.running_mean\", \"embedding.end_block1.0.running_var\", \"embedding.end_block1.0.num_batches_tracked\", \"embedding.end_block2.0.weight\", \"embedding.end_block2.0.bias\", \"embedding.end_block2.1.weight\", \"embedding.end_block2.1.bias\", \"embedding.end_block2.1.running_mean\", \"embedding.end_block2.1.running_var\", \"embedding.end_block2.1.num_batches_tracked\", \"cos.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6ba10fe94147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/detection/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tMissing key(s) in state_dict: \"res0.weight\", \"res1.0.fx.0.weight\", \"res1.0.fx.0.bias\", \"res1.0.fx.0.running_mean\", \"res1.0.fx.0.running_var\", \"res1.0.fx.1.weight\", \"res1.0.fx.2.weight\", \"res1.0.fx.3.weight\", \"res1.0.gx.0.weight\", \"res1.1.fx.0.weight\", \"res1.1.fx.0.bias\", \"res1.1.fx.0.running_mean\", \"res1.1.fx.0.running_var\", \"res1.1.fx.1.weight\", \"res1.1.fx.2.weight\", \"res1.1.fx.3.weight\", \"res1.2.fx.0.weight\", \"res1.2.fx.0.bias\", \"res1.2.fx.0.running_mean\", \"res1.2.fx.0.running_var\", \"res1.2.fx.1.weight\", \"res1.2.fx.2.weight\", \"res1.2.fx.3.weight\", \"res2.0.fx.0.weight\", \"res2.0.fx.0.bias\", \"res2.0.fx.0.running_mean\", \"res2.0.fx.0.running_var\", \"res2.0.fx.1.weight\", \"res2.0.fx.2.weight\", \"res2.0.fx.3.weight\", \"res2.0.gx.0.weight\", \"res2.1.fx.0.weight\", \"res2.1.fx.0.bias\", \"res2.1.fx.0.running_mean\", \"res2.1.fx.0.running_var\", \"res2.1.fx.1.weight\", \"res2.1.fx.2.weight\", \"res2.1.fx.3.weight\", \"res2.2.fx.0.weight\", \"res2.2.fx.0.bias\", \"res2.2.fx.0.running_mean\", \"res2.2.fx.0.running_var\", \"res2.2.fx.1.weight\", \"res2.2.fx.2.weight\", \"res2.2.fx.3.weight\", \"res2.3.fx.0.weight\", \"res2.3.fx.0.bias\", \"res2.3.fx.0.running_mean\", \"res2.3.fx.0.running_var\", \"res2.3.fx.1.weight\", \"res2.3.fx.2.weight\", \"res2.3.fx.3.weight\", \"res3.0.fx.0.weight\", \"res3.0.fx.0.bias\", \"res3.0.fx.0.running_mean\", \"res3.0.fx.0.running_var\", \"res3.0.fx.1.weight\", \"res3.0.fx.2.weight\", \"res3.0.fx.3.weight\", \"res3.0.gx.0.weight\", \"res3.1.fx.0.weight\", \"res3.1.fx.0.bias\", \"res3.1.fx.0.running_mean\", \"res3.1.fx.0.running_var\", \"res3.1.fx.1.weight\", \"res3.1.fx.2.weight\", \"res3.1.fx.3.weight\", \"res3.2.fx.0.weight\", \"res3.2.fx.0.bias\", \"res3.2.fx.0.running_mean\", \"res3.2.fx.0.running_var\", \"res3.2.fx.1.weight\", \"res3.2.fx.2.weight\", \"res3.2.fx.3.weight\", \"res3.3.fx.0.weight\", \"res3.3.fx.0.bias\", \"res3.3.fx.0.running_mean\", \"res3.3.fx.0.running_var\", \"res3.3.fx.1.weight\", \"res3.3.fx.2.weight\", \"res3.3.fx.3.weight\", \"res3.4.fx.0.weight\", \"res3.4.fx.0.bias\", \"res3.4.fx.0.running_mean\", \"res3.4.fx.0.running_var\", \"res3.4.fx.1.weight\", \"res3.4.fx.2.weight\", \"res3.4.fx.3.weight\", \"res3.5.fx.0.weight\", \"res3.5.fx.0.bias\", \"res3.5.fx.0.running_mean\", \"res3.5.fx.0.running_var\", \"res3.5.fx.1.weight\", \"res3.5.fx.2.weight\", \"res3.5.fx.3.weight\", \"res3.6.fx.0.weight\", \"res3.6.fx.0.bias\", \"res3.6.fx.0.running_mean\", \"res3.6.fx.0.running_var\", \"res3.6.fx.1.weight\", \"res3.6.fx.2.weight\", \"res3.6.fx.3.weight\", \"res3.7.fx.0.weight\", \"res3.7.fx.0.bias\", \"res3.7.fx.0.running_mean\", \"res3.7.fx.0.running_var\", \"res3.7.fx.1.weight\", \"res3.7.fx.2.weight\", \"res3.7.fx.3.weight\", \"res3.8.fx.0.weight\", \"res3.8.fx.0.bias\", \"res3.8.fx.0.running_mean\", \"res3.8.fx.0.running_var\", \"res3.8.fx.1.weight\", \"res3.8.fx.2.weight\", \"res3.8.fx.3.weight\", \"res3.9.fx.0.weight\", \"res3.9.fx.0.bias\", \"res3.9.fx.0.running_mean\", \"res3.9.fx.0.running_var\", \"res3.9.fx.1.weight\", \"res3.9.fx.2.weight\", \"res3.9.fx.3.weight\", \"res3.10.fx.0.weight\", \"res3.10.fx.0.bias\", \"res3.10.fx.0.running_mean\", \"res3.10.fx.0.running_var\", \"res3.10.fx.1.weight\", \"res3.10.fx.2.weight\", \"res3.10.fx.3.weight\", \"res3.11.fx.0.weight\", \"res3.11.fx.0.bias\", \"res3.11.fx.0.running_mean\", \"res3.11.fx.0.running_var\", \"res3.11.fx.1.weight\", \"res3.11.fx.2.weight\", \"res3.11.fx.3.weight\", \"res3.12.fx.0.weight\", \"res3.12.fx.0.bias\", \"res3.12.fx.0.running_mean\", \"res3.12.fx.0.running_var\", \"res3.12.fx.1.weight\", \"res3.12.fx.2.weight\", \"res3.12.fx.3.weight\", \"res3.13.fx.0.weight\", \"res3.13.fx.0.bias\", \"res3.13.fx.0.running_mean\", \"res3.13.fx.0.running_var\", \"res3.13.fx.1.weight\", \"res3.13.fx.2.weight\", \"res3.13.fx.3.weight\", \"res4.0.fx.0.weight\", \"res4.0.fx.0.bias\", \"res4.0.fx.0.running_mean\", \"res4.0.fx.0.running_var\", \"res4.0.fx.1.weight\", \"res4.0.fx.2.weight\", \"res4.0.fx.3.weight\", \"res4.0.gx.0.weight\", \"res4.1.fx.0.weight\", \"res4.1.fx.0.bias\", \"res4.1.fx.0.running_mean\", \"res4.1.fx.0.running_var\", \"res4.1.fx.1.weight\", \"res4.1.fx.2.weight\", \"res4.1.fx.3.weight\", \"res4.2.fx.0.weight\", \"res4.2.fx.0.bias\", \"res4.2.fx.0.running_mean\", \"res4.2.fx.0.running_var\", \"res4.2.fx.1.weight\", \"res4.2.fx.2.weight\", \"res4.2.fx.3.weight\", \"end_block1.0.weight\", \"end_block1.0.bias\", \"end_block1.0.running_mean\", \"end_block1.0.running_var\", \"end_block2.0.weight\", \"end_block2.0.bias\", \"end_block2.1.weight\", \"end_block2.1.bias\", \"end_block2.1.running_mean\", \"end_block2.1.running_var\". \n\tUnexpected key(s) in state_dict: \"embedding.res0.weight\", \"embedding.res1.0.fx.0.weight\", \"embedding.res1.0.fx.0.bias\", \"embedding.res1.0.fx.0.running_mean\", \"embedding.res1.0.fx.0.running_var\", \"embedding.res1.0.fx.0.num_batches_tracked\", \"embedding.res1.0.fx.1.weight\", \"embedding.res1.0.fx.3.weight\", \"embedding.res1.0.gx.0.weight\", \"embedding.res1.1.fx.0.weight\", \"embedding.res1.1.fx.0.bias\", \"embedding.res1.1.fx.0.running_mean\", \"embedding.res1.1.fx.0.running_var\", \"embedding.res1.1.fx.0.num_batches_tracked\", \"embedding.res1.1.fx.1.weight\", \"embedding.res1.1.fx.3.weight\", \"embedding.res1.2.fx.0.weight\", \"embedding.res1.2.fx.0.bias\", \"embedding.res1.2.fx.0.running_mean\", \"embedding.res1.2.fx.0.running_var\", \"embedding.res1.2.fx.0.num_batches_tracked\", \"embedding.res1.2.fx.1.weight\", \"embedding.res1.2.fx.3.weight\", \"embedding.res2.0.fx.0.weight\", \"embedding.res2.0.fx.0.bias\", \"embedding.res2.0.fx.0.running_mean\", \"embedding.res2.0.fx.0.running_var\", \"embedding.res2.0.fx.0.num_batches_tracked\", \"embedding.res2.0.fx.1.weight\", \"embedding.res2.0.fx.3.weight\", \"embedding.res2.0.gx.0.weight\", \"embedding.res2.1.fx.0.weight\", \"embedding.res2.1.fx.0.bias\", \"embedding.res2.1.fx.0.running_mean\", \"embedding.res2.1.fx.0.running_var\", \"embedding.res2.1.fx.0.num_batches_tracked\", \"embedding.res2.1.fx.1.weight\", \"embedding.res2.1.fx.3.weight\", \"embedding.res2.2.fx.0.weight\", \"embedding.res2.2.fx.0.bias\", \"embedding.res2.2.fx.0.running_mean\", \"embedding.res2.2.fx.0.running_var\", \"embedding.res2.2.fx.0.num_batches_tracked\", \"embedding.res2.2.fx.1.weight\", \"embedding.res2.2.fx.3.weight\", \"embedding.res2.3.fx.0.weight\", \"embedding.res2.3.fx.0.bias\", \"embedding.res2.3.fx.0.running_mean\", \"embedding.res2.3.fx.0.running_var\", \"embedding.res2.3.fx.0.num_batches_tracked\", \"embedding.res2.3.fx.1.weight\", \"embedding.res2.3.fx.3.weight\", \"embedding.res3.0.fx.0.weight\", \"embedding.res3.0.fx.0.bias\", \"embedding.res3.0.fx.0.running_mean\", \"embedding.res3.0.fx.0.running_var\", \"embedding.res3.0.fx.0.num_batches_tracked\", \"embedding.res3.0.fx.1.weight\", \"embedding.res3.0.fx.3.weight\", \"embedding.res3.0.gx.0.weight\", \"embedding.res3.1.fx.0.weight\", \"embedding.res3.1.fx.0.bias\", \"embedding.res3.1.fx.0.running_mean\", \"embedding.res3.1.fx.0.running_var\", \"embedding.res3.1.fx.0.num_batches_tracked\", \"embedding.res3.1.fx.1.weight\", \"embedding.res3.1.fx.3.weight\", \"embedding.res3.2.fx.0.weight\", \"embedding.res3.2.fx.0.bias\", \"embedding.res3.2.fx.0.running_mean\", \"embedding.res3.2.fx.0.running_var\", \"embedding.res3.2.fx.0.num_batches_tracked\", \"embedding.res3.2.fx.1.weight\", \"embedding.res3.2.fx.3.weight\", \"embedding.res3.3.fx.0.weight\", \"embedding.res3.3.fx.0.bias\", \"embedding.res3.3.fx.0.running_mean\", \"embedding.res3.3.fx.0.running_var\", \"embedding.res3.3.fx.0.num_batches_tracked\", \"embedding.res3.3.fx.1.weight\", \"embedding.res3.3.fx.3.weight\", \"embedding.res3.4.fx.0.weight\", \"embedding.res3.4.fx.0.bias\", \"embedding.res3.4.fx.0.running_mean\", \"embedding.res3.4.fx.0.running_var\", \"embedding.res3.4.fx.0.num_batches_tracked\", \"embedding.res3.4.fx.1.weight\", \"embedding.res3.4.fx.3.weight\", \"embedding.res3.5.fx.0.weight\", \"embedding.res3.5.fx.0.bias\", \"embedding.res3.5.fx.0.running_mean\", \"embedding.res3.5.fx.0.running_var\", \"embedding.res3.5.fx.0.num_batches_tracked\", \"embedding.res3.5.fx.1.weight\", \"embedding.res3.5.fx.3.weight\", \"embedding.res3.6.fx.0.weight\", \"embedding.res3.6.fx.0.bias\", \"embedding.res3.6.fx.0.running_mean\", \"embedding.res3.6.fx.0.running_var\", \"embedding.res3.6.fx.0.num_batches_tracked\", \"embedding.res3.6.fx.1.weight\", \"embedding.res3.6.fx.3.weight\", \"embedding.res3.7.fx.0.weight\", \"embedding.res3.7.fx.0.bias\", \"embedding.res3.7.fx.0.running_mean\", \"embedding.res3.7.fx.0.running_var\", \"embedding.res3.7.fx.0.num_batches_tracked\", \"embedding.res3.7.fx.1.weight\", \"embedding.res3.7.fx.3.weight\", \"embedding.res3.8.fx.0.weight\", \"embedding.res3.8.fx.0.bias\", \"embedding.res3.8.fx.0.running_mean\", \"embedding.res3.8.fx.0.running_var\", \"embedding.res3.8.fx.0.num_batches_tracked\", \"embedding.res3.8.fx.1.weight\", \"embedding.res3.8.fx.3.weight\", \"embedding.res3.9.fx.0.weight\", \"embedding.res3.9.fx.0.bias\", \"embedding.res3.9.fx.0.running_mean\", \"embedding.res3.9.fx.0.running_var\", \"embedding.res3.9.fx.0.num_batches_tracked\", \"embedding.res3.9.fx.1.weight\", \"embedding.res3.9.fx.3.weight\", \"embedding.res3.10.fx.0.weight\", \"embedding.res3.10.fx.0.bias\", \"embedding.res3.10.fx.0.running_mean\", \"embedding.res3.10.fx.0.running_var\", \"embedding.res3.10.fx.0.num_batches_tracked\", \"embedding.res3.10.fx.1.weight\", \"embedding.res3.10.fx.3.weight\", \"embedding.res3.11.fx.0.weight\", \"embedding.res3.11.fx.0.bias\", \"embedding.res3.11.fx.0.running_mean\", \"embedding.res3.11.fx.0.running_var\", \"embedding.res3.11.fx.0.num_batches_tracked\", \"embedding.res3.11.fx.1.weight\", \"embedding.res3.11.fx.3.weight\", \"embedding.res3.12.fx.0.weight\", \"embedding.res3.12.fx.0.bias\", \"embedding.res3.12.fx.0.running_mean\", \"embedding.res3.12.fx.0.running_var\", \"embedding.res3.12.fx.0.num_batches_tracked\", \"embedding.res3.12.fx.1.weight\", \"embedding.res3.12.fx.3.weight\", \"embedding.res3.13.fx.0.weight\", \"embedding.res3.13.fx.0.bias\", \"embedding.res3.13.fx.0.running_mean\", \"embedding.res3.13.fx.0.running_var\", \"embedding.res3.13.fx.0.num_batches_tracked\", \"embedding.res3.13.fx.1.weight\", \"embedding.res3.13.fx.3.weight\", \"embedding.res4.0.fx.0.weight\", \"embedding.res4.0.fx.0.bias\", \"embedding.res4.0.fx.0.running_mean\", \"embedding.res4.0.fx.0.running_var\", \"embedding.res4.0.fx.0.num_batches_tracked\", \"embedding.res4.0.fx.1.weight\", \"embedding.res4.0.fx.3.weight\", \"embedding.res4.0.gx.0.weight\", \"embedding.res4.1.fx.0.weight\", \"embedding.res4.1.fx.0.bias\", \"embedding.res4.1.fx.0.running_mean\", \"embedding.res4.1.fx.0.running_var\", \"embedding.res4.1.fx.0.num_batches_tracked\", \"embedding.res4.1.fx.1.weight\", \"embedding.res4.1.fx.3.weight\", \"embedding.res4.2.fx.0.weight\", \"embedding.res4.2.fx.0.bias\", \"embedding.res4.2.fx.0.running_mean\", \"embedding.res4.2.fx.0.running_var\", \"embedding.res4.2.fx.0.num_batches_tracked\", \"embedding.res4.2.fx.1.weight\", \"embedding.res4.2.fx.3.weight\", \"embedding.end_block1.0.weight\", \"embedding.end_block1.0.bias\", \"embedding.end_block1.0.running_mean\", \"embedding.end_block1.0.running_var\", \"embedding.end_block1.0.num_batches_tracked\", \"embedding.end_block2.0.weight\", \"embedding.end_block2.0.bias\", \"embedding.end_block2.1.weight\", \"embedding.end_block2.1.bias\", \"embedding.end_block2.1.running_mean\", \"embedding.end_block2.1.running_var\", \"embedding.end_block2.1.num_batches_tracked\", \"cos.weight\". "
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "model.load_state_dict(a['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = InsightFace(85164)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.load_state_dict(a['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-b6e645d65584>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/detection/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "mod.embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
